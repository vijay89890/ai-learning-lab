# ğŸ§  AI Learning Lab
Hands-on experiments while I learn emerging AI concepts: **Prompt Engineering**, **Chain-of-Thought / Tree-of-Thought**, and **Retrieval-Augmented Generation (RAG)**.

> ğŸ¯ Goal: Learn by building. Each folder contains runnable notebooks, clear explanations, and small milestones so I can demonstrate progress in public.

---

## ğŸ” Whatâ€™s inside
- `prompting-basics/` â€” Zero-shot vs few-shot prompting, prompt patterns, and evaluation.
- `cot-tot/` â€” Chain-of-Thought (CoT) and a tiny Tree-of-Thought (ToT) exploration.
- `rag-demo/` â€” Minimal RAG pipeline (chunking â†’ embeddings â†’ vector search â†’ answer), with a simple UI option later.

---

## ğŸš€ Quickstart
1) **Clone & set up**
```bash
git clone https://github.com/<your-username>/ai-learning-lab.git
cd ai-learning-lab

# (Recommended) Create a virtual environment
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate

pip install -r requirements.txt
```

2) **Set your API keys (optional, for LLM responses)**  
Copy `.env.example` â†’ `.env` and fill your values:
```
OPENAI_API_KEY=your_key_here
```

3) **Run notebooks**
```bash
python -m ipykernel install --user --name ai-learning-lab
jupyter lab  # or jupyter notebook
```

Open the notebooks in each folder and run top to bottom.

---

## ğŸ“š Project Structure
```
ai-learning-lab/
â”‚â”€â”€ README.md
â”‚â”€â”€ ROADMAP.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore
â”‚â”€â”€ .env.example
â”‚
â”œâ”€â”€ prompting-basics/
â”‚   â””â”€â”€ zero_vs_few_shot.ipynb
â”‚
â”œâ”€â”€ cot-tot/
â”‚   â””â”€â”€ reasoning_methods.ipynb
â”‚
â””â”€â”€ rag-demo/
    â”œâ”€â”€ minimal_rag.ipynb
    â””â”€â”€ sample_data/
        â”œâ”€â”€ rag_note_1.txt
        â””â”€â”€ rag_note_2.txt
```

---

## ğŸ§© Notebooks Overview
### 1) Prompting Basics
- Compare **zero-shot** vs **few-shot** on the same task.
- Try prompt patterns: role prompting, format constraints, and style guides.
- Keep results **side-by-side** and write short observations.

### 2) CoT & ToT
- Show a normal answer vs **CoT** (â€œthink step-by-stepâ€).
- Tiny **ToT** experiment: explore multiple reasoning branches and pick a best one.

### 3) RAG Demo
- Parse text files, **chunk**, compute **embeddings**, index with **FAISS**, and **retrieve** top-k chunks.
- Compose a final answer with retrieved context.
- Works without an API key (prints retrieved context), or uses an LLM if you configure `OPENAI_API_KEY`.

---

## âœ… How to Show Progress on GitHub
- Use clear commit messages:
  - `feat(prompting): add zero vs few-shot comparison`
  - `chore(rag): add sample_data and chunker util`
- Add screenshots/GIFs of notebook outputs in the README (drag & drop into GitHub issues to get image URLs).

---

## ğŸ—ºï¸ Roadmap (high-level)
- âœ… Stage 1: Minimal notebooks for prompting, CoT/ToT, and RAG
- â¬œ Stage 2: Add evaluation (simple metrics + qualitative notes)
- â¬œ Stage 3: RAG UI with Streamlit/Gradio (+ deployment)
- â¬œ Stage 4: Agents & n8n workflows (alerts, scheduled crawls)

See **ROADMAP.md** for details and tasks.

---

## âš™ï¸ Requirements
See `requirements.txt`. Notebooks try to install missing libs where possible.

> â„¹ï¸ If you donâ€™t have API keys, you can still run everything. The RAG notebook will print retrieved context and a template answer without LLMs.

---

## ğŸ“ License
MIT â€” feel free to fork, learn, and build on top.