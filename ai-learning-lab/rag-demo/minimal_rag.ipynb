{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf3410d",
   "metadata": {},
   "source": [
    "\n",
    "# Minimal RAG: chunk → embed → index (FAISS) → retrieve → answer\n",
    "\n",
    "**What you'll do**\n",
    "1. Load sample text files from `sample_data/`\n",
    "2. Chunk the text\n",
    "3. Compute embeddings with `sentence-transformers`\n",
    "4. Index vectors in FAISS and retrieve top-k chunks for a query\n",
    "5. Compose an answer (with or without an LLM)\n",
    "\n",
    "> Works offline (no API key) for retrieval; adds LLM answer if `OPENAI_API_KEY` is set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff668b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in fresh envs, you can install these here:\n",
    "# %pip install -q sentence-transformers faiss-cpu numpy pandas python-dotenv openai\n",
    "\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Optional LLM\n",
    "USE_OPENAI = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "if USE_OPENAI:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "# 1) Load docs\n",
    "def load_docs(path=\"sample_data/*.txt\"):\n",
    "    docs = []\n",
    "    for p in glob.glob(path):\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "docs = load_docs()\n",
    "print(f\"Loaded {len(docs)} docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ad6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Chunking (simple paragraph/sentence splits)\n",
    "def simple_chunk(text, max_chars=500):\n",
    "    # split by double newline first, then by sentence-ish punctuation\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        buf = \"\"\n",
    "        for sent in re.split(r'(?<=[.!?])\\s+', para):\n",
    "            if len(buf) + len(sent) <= max_chars:\n",
    "                buf += (\" \" if buf else \"\") + sent\n",
    "            else:\n",
    "                if buf:\n",
    "                    chunks.append(buf.strip())\n",
    "                buf = sent\n",
    "        if buf:\n",
    "            chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for d in docs:\n",
    "    all_chunks.extend(simple_chunk(d, max_chars=400))\n",
    "\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574acef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Embeddings\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(all_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "embeddings = embeddings.astype(\"float32\")\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a798635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) FAISS index (Inner Product works with normalized vectors for cosine similarity)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "print(\"Indexed vectors:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Retrieval\n",
    "def retrieve(query, k=5):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    results = [(float(scores[0][i]), all_chunks[int(idxs[0][i])]) for i in range(k)]\n",
    "    return results\n",
    "\n",
    "query = \"What is RAG and why is it useful?\"\n",
    "hits = retrieve(query, k=4)\n",
    "for s, ch in hits:\n",
    "    print(f\"\\n[score={s:.3f}] {ch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e445a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Compose final answer (with/without LLM)\n",
    "context = \"\\n\\n\".join([ch for _, ch in hits])\n",
    "\n",
    "if client is None:\n",
    "    print(\"\\n=== Template Answer (no LLM) ===\")\n",
    "    print(\"Question:\", query)\n",
    "    print(\"Key points from retrieved context:\")\n",
    "    print(\"-\", \"\\n- \".join([ch[:180] + (\"...\" if len(ch) > 180 else \"\") for _, ch in hits]))\n",
    "else:\n",
    "    prompt = f\"\"\"Use the context to answer the question accurately.\n",
    "If something isn't in the context, say so briefly.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"system\",\"content\":\"You answer strictly based on the given context.\"},\n",
    "                  {\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    print(\"\\n=== LLM Answer ===\\n\")\n",
    "    print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b35ba",
   "metadata": {},
   "source": [
    "\n",
    "### Experiments to try next\n",
    "- Change `max_chars` in chunking and compare retrieval quality.\n",
    "- Try different k values in `retrieve()`.\n",
    "- Swap embedding model (e.g., `all-mpnet-base-v2`) and compare.\n",
    "- Add `Streamlit` UI: a textbox for query + show retrieved chunks + answer.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
